# Router Configuration
# Configure model routing for different complexity levels

# Server settings
server:
  host: "127.0.0.1"
  port: 4001  # Default port, can be overridden with --port

# Classifier settings
# provider: "local" (Ollama, free), "anthropic", or "openai"
classifier:
  provider: "local"  # Use "anthropic" or "openai" if you can't run local models
  model: "qwen2.5:3b"  # local: any Ollama model, anthropic: claude-haiku-*, openai: gpt-4o-mini
  ollama_url: "http://localhost:11434/api/generate"
  routes_file: "ROUTES.md"

# Tool routing - controls model selection when tools are present
# Some recommend using more capable models for tool use to reduce prompt injection risks
tools:
  # Option 1: Set minimum complexity floor when tools present
  min_complexity: "easy"  # "easy", "medium", "hard", or "super_hard"

  # Option 2: Force specific model for ALL tool calls (overrides min_complexity)
  # model: "anthropic:claude-opus-4-20250514"

  # If neither set, defaults to bumping super_easy -> easy

# Model routing by complexity level
# Format: "provider:model" where provider is: local, anthropic, openai, google
#
# Providers:
#   - local: Ollama models (free, runs locally)
#   - anthropic: Claude models (requires ANTHROPIC_API_KEY)
#   - openai: GPT/o-series models (requires OPENAI_API_KEY)
#   - google: Gemini models (requires GOOGLE_API_KEY)
#
# OpenAI models:
#   - gpt-4o, gpt-4o-mini: Fast, multimodal
#   - o1, o3, o4-mini: Reasoning models (auto-detected, uses different API params)
#
models:
  super_easy: "anthropic:claude-haiku-4-5-20251001" # Fast, cheap
  easy: "anthropic:claude-sonnet-4-20250514"        # Balanced
  medium: "anthropic:claude-sonnet-4-20250514"      # Balanced
  hard: "anthropic:claude-opus-4-20250514"          # Powerful
  super_hard: "anthropic:claude-opus-4-20250514"    # Most capable

# Alternative: OpenAI-only routing
# models:
#   super_easy: "openai:gpt-4o-mini"
#   easy: "openai:gpt-4o-mini"
#   medium: "openai:gpt-4o"
#   hard: "openai:o3"           # Reasoning model
#   super_hard: "openai:o3"     # Reasoning model

# Provider API endpoints and keys
# api_key: Set per provider. If not set, falls back to request Authorization header.
providers:
  anthropic:
    url: "https://api.anthropic.com/v1/messages"
    version: "2023-06-01"
    # api_key: "sk-ant-..."  # API key or OAuth token
  openai:
    url: "https://api.openai.com/v1/chat/completions"
    # api_key: "sk-proj-..."
  google:
    url: "https://generativelanguage.googleapis.com/v1beta/models"
    # api_key: "..."
  deepseek:
    url: "https://api.deepseek.com/v1/chat/completions"
    # api_key: "sk-..."
  kimi:
    url: "https://api.moonshot.cn/v1/chat/completions"
    # api_key: "sk-..."
  ollama:
    url: "http://localhost:11434/api/chat"
